{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S89AJpQYG3du"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wlnQdsrDh9AX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape = (60000, 28, 28, 1), y_train.shape = (60000, 10)\n",
      "x_test.shape = (10000, 28, 28, 1), y_test.shape = (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data(path='mnist.npz')\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], x_test.shape[2], 1)\n",
    "\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "y_train = to_categorical(y_train, NUM_CLASSES)\n",
    "y_test = to_categorical(y_test, NUM_CLASSES)\n",
    "\n",
    "print(\"x_train.shape = {}, y_train.shape = {}\".format(x_train.shape, y_train.shape))\n",
    "print(\"x_test.shape = {}, y_test.shape = {}\".format(x_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Lq0YDUYiTMN"
   },
   "outputs": [],
   "source": [
    "inputs = Input(shape=(28, 28, 1), name='input')\n",
    "\n",
    "x = Conv2D(24, kernel_size=(6, 6), strides=1)(inputs)\n",
    "x = BatchNormalization(scale=False, beta_initializer=Constant(0.01))(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dropout(rate=0.25)(x)\n",
    "\n",
    "x = Conv2D(48, kernel_size=(5, 5), strides=2)(x)\n",
    "x = BatchNormalization(scale=False, beta_initializer=Constant(0.01))(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dropout(rate=0.25)(x)\n",
    "\n",
    "x = Conv2D(64, kernel_size=(4, 4), strides=2)(x)\n",
    "x = BatchNormalization(scale=False, beta_initializer=Constant(0.01))(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dropout(rate=0.25)(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Dense(200)(x)\n",
    "x = BatchNormalization(scale=False, beta_initializer=Constant(0.01))(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dropout(rate=0.25)(x)\n",
    "\n",
    "predications = Dense(NUM_CLASSES, activation='softmax', name='output')(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=predications)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nBzYWAEAiwzx",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 23, 23, 24)        888       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 23, 23, 24)       72        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " activation (Activation)     (None, 23, 23, 24)        0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 23, 23, 24)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 10, 10, 48)        28848     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 10, 10, 48)       144       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 10, 10, 48)        0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 10, 10, 48)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 4, 4, 64)          49216     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 4, 4, 64)         192       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 4, 4, 64)          0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 4, 4, 64)          0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1024)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 200)               205000    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 200)              600       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 200)               0         \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 200)               0         \n",
      "                                                                 \n",
      " output (Dense)              (None, 10)                2010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 286,970\n",
      "Trainable params: 286,298\n",
      "Non-trainable params: 672\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "McycGoh0itz2",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0201.\n",
      "Epoch 1/20\n",
      "469/469 [==============================] - 170s 364ms/step - loss: 0.0929 - accuracy: 0.9710 - val_loss: 0.0458 - val_accuracy: 0.9848 - lr: 0.0201\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.014430626211475785.\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 161s 344ms/step - loss: 0.0573 - accuracy: 0.9824 - val_loss: 0.0330 - val_accuracy: 0.9883 - lr: 0.0144\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.01036834238065184.\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 161s 344ms/step - loss: 0.0388 - accuracy: 0.9881 - val_loss: 0.0224 - val_accuracy: 0.9928 - lr: 0.0104\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.007457588823428847.\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 178s 380ms/step - loss: 0.0325 - accuracy: 0.9899 - val_loss: 0.0235 - val_accuracy: 0.9928 - lr: 0.0075\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.005371942762314537.\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 211s 451ms/step - loss: 0.0250 - accuracy: 0.9921 - val_loss: 0.0187 - val_accuracy: 0.9938 - lr: 0.0054\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.0038775120567512366.\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 201s 428ms/step - loss: 0.0203 - accuracy: 0.9934 - val_loss: 0.0184 - val_accuracy: 0.9948 - lr: 0.0039\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.002806705664732254.\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 180s 384ms/step - loss: 0.0183 - accuracy: 0.9946 - val_loss: 0.0157 - val_accuracy: 0.9947 - lr: 0.0028\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.002039439357288101.\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 171s 366ms/step - loss: 0.0152 - accuracy: 0.9951 - val_loss: 0.0157 - val_accuracy: 0.9954 - lr: 0.0020\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.0014896690244560313.\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 221s 471ms/step - loss: 0.0146 - accuracy: 0.9952 - val_loss: 0.0155 - val_accuracy: 0.9945 - lr: 0.0015\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.001095741367357279.\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 166s 353ms/step - loss: 0.0133 - accuracy: 0.9957 - val_loss: 0.0139 - val_accuracy: 0.9954 - lr: 0.0011\n",
      "\n",
      "Epoch 11: LearningRateScheduler setting learning rate to 0.000813479866945048.\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 134s 285ms/step - loss: 0.0114 - accuracy: 0.9963 - val_loss: 0.0142 - val_accuracy: 0.9951 - lr: 8.1348e-04\n",
      "\n",
      "Epoch 12: LearningRateScheduler setting learning rate to 0.0006112306641301482.\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 142s 302ms/step - loss: 0.0118 - accuracy: 0.9962 - val_loss: 0.0140 - val_accuracy: 0.9950 - lr: 6.1123e-04\n",
      "\n",
      "Epoch 13: LearningRateScheduler setting learning rate to 0.00046631277777468366.\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 134s 285ms/step - loss: 0.0099 - accuracy: 0.9968 - val_loss: 0.0144 - val_accuracy: 0.9953 - lr: 4.6631e-04\n",
      "\n",
      "Epoch 14: LearningRateScheduler setting learning rate to 0.00036247457473881936.\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 134s 285ms/step - loss: 0.0097 - accuracy: 0.9968 - val_loss: 0.0142 - val_accuracy: 0.9955 - lr: 3.6247e-04\n",
      "\n",
      "Epoch 15: LearningRateScheduler setting learning rate to 0.00028807125102990415.\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 133s 283ms/step - loss: 0.0101 - accuracy: 0.9966 - val_loss: 0.0141 - val_accuracy: 0.9954 - lr: 2.8807e-04\n",
      "\n",
      "Epoch 16: LearningRateScheduler setting learning rate to 0.0002347589399817094.\n",
      "Epoch 16/20\n",
      "469/469 [==============================] - 133s 283ms/step - loss: 0.0090 - accuracy: 0.9971 - val_loss: 0.0138 - val_accuracy: 0.9954 - lr: 2.3476e-04\n",
      "\n",
      "Epoch 17: LearningRateScheduler setting learning rate to 0.00019655899987662886.\n",
      "Epoch 17/20\n",
      "469/469 [==============================] - 142s 303ms/step - loss: 0.0095 - accuracy: 0.9969 - val_loss: 0.0136 - val_accuracy: 0.9952 - lr: 1.9656e-04\n",
      "\n",
      "Epoch 18: LearningRateScheduler setting learning rate to 0.0001691875467292952.\n",
      "Epoch 18/20\n",
      "469/469 [==============================] - 142s 302ms/step - loss: 0.0086 - accuracy: 0.9971 - val_loss: 0.0139 - val_accuracy: 0.9953 - lr: 1.6919e-04\n",
      "\n",
      "Epoch 19: LearningRateScheduler setting learning rate to 0.0001495750435333272.\n",
      "Epoch 19/20\n",
      "469/469 [==============================] - 132s 283ms/step - loss: 0.0088 - accuracy: 0.9973 - val_loss: 0.0143 - val_accuracy: 0.9952 - lr: 1.4958e-04\n",
      "\n",
      "Epoch 20: LearningRateScheduler setting learning rate to 0.0001355220709146876.\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 132s 281ms/step - loss: 0.0084 - accuracy: 0.9974 - val_loss: 0.0143 - val_accuracy: 0.9951 - lr: 1.3552e-04\n"
     ]
    }
   ],
   "source": [
    "lr_decay = lambda epoch: 0.0001 + 0.02 * math.pow(1.0 / math.e, epoch / 3.0)\n",
    "decay_callback = LearningRateScheduler(lr_decay, verbose=1)\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=128, epochs=20, verbose=1, \n",
    "                    validation_data=(x_test, y_test), callbacks=[decay_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FinX93e1jKz9"
   },
   "outputs": [],
   "source": [
    "model.save('myhand_CNN_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ioCGtGB3mkDv",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\dudgm\\AppData\\Local\\Temp\\tmpj_9esero\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\dudgm\\AppData\\Local\\Temp\\tmpj_9esero\\assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1147780"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#converter = tf.lite.TFLiteConverter.from_keras_model_file(model)\n",
    "#tflite_model = converter.convert()\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "open('mnist.tflite', 'wb').write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "51PTkdoPDOTW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip downloading\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from google.colab import files\n",
    "    files.download('mnist.tflite')\n",
    "except:\n",
    "    print(\"Skip downloading\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "mnist_batchnorm.ipynb",
   "private_outputs": true,
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
